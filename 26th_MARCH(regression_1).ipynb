{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### `Q1`. Explain the difference between simple linear regression and multiple linear regression. Provide an example of each.\n"
      ],
      "metadata": {
        "id": "726lR5QEWoS3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Simple linear regression and multiple linear regression is the number of independent variables being used to predict the dependent variable. \n",
        "* Simple linear regression uses only one independent variable, while multiple linear regression uses two or more.\n",
        "\n",
        "  - Example of simple linear regression: Suppose we want to predict the price of a house based on its square footage. We can use a simple linear regression model with square footage as the independent variable and the house price as the dependent variable.\n",
        "\n",
        "  - Example of multiple linear regression: Suppose we want to predict a person's blood pressure based on their age, weight, and cholesterol level. We can use a multiple linear regression model with age, weight, and cholesterol level as the independent variables and blood pressure as the dependent variable."
      ],
      "metadata": {
        "id": "dJXavXjSYeWd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### `Q2`. Discuss the assumptions of linear regression. How can you check whether these assumptions hold in a given dataset?\n",
        "\n"
      ],
      "metadata": {
        "id": "GkLOtOthXA_n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Linear regression makes several assumptions about the relationship between the independent variables and the dependent variable. These assumptions include:\n",
        "\n",
        "1. **Linearity:** The relationship between the independent variables and the dependent variable is linear.\n",
        "\n",
        "2. **Independence:** The observations are independent of each other.\n",
        "\n",
        "3. **Homoscedasticity:** The variance of the errors is constant across all levels of the independent variables.\n",
        "\n",
        "4. **Normality:** The errors are normally distributed.\n",
        "\n",
        "5. **No multicollinearity:** The independent variables are not highly correlated with each other.\n",
        "\n",
        "* To check whether these assumptions hold in a given dataset, several diagnostic tests can be performed. These tests include:\n",
        "\n",
        "  - Scatter plots: Scatter plots can be used to check the linearity assumption. If the relationship between the independent variables and the dependent variable is not linear, then linear regression may not be appropriate.\n",
        "\n",
        "  - Residual plots: Residual plots can be used to check the homoscedasticity assumption. If the variance of the errors is not constant across all levels of the independent variables, then linear regression may not be appropriate.\n",
        "\n"
      ],
      "metadata": {
        "id": "NDwdn79oZgPA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### `Q3`. How do you interpret the slope and intercept in a linear regression model? Provide an example using a real-world scenario.\n",
        "\n"
      ],
      "metadata": {
        "id": "3XKfXUwNZUgJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The slope and intercept in a linear regression model represent the relationship between the independent variable and the dependent variable. \n",
        "* The intercept represents the value of the dependent variable when the independent variable is zero.\n",
        "* slope represents the change in the dependent variable for a one-unit change in the independent variable.\n",
        "*  Example:\n",
        "  - consider a linear regression model that predicts a person's weight based on their height. If the slope of the model is 3, this means that for every one-inch increase in height, the predicted weight increases by 3 pounds. The intercept represents the weight of a person who has a height of zero, which is not meaningful in this context."
      ],
      "metadata": {
        "id": "Lb3Ycuc8cypD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### `Q4`. Explain the concept of gradient descent. How is it used in machine earning?\n",
        "\n"
      ],
      "metadata": {
        "id": "4tjX9YW-ZXy3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Gradient descent is a first-order iterative optimization algorithm used to find the minimum of a function. It works by iteratively adjusting the parameters of the function in the direction of the negative gradient until a minimum is reached.\n",
        "\n",
        "* In machine learning, gradient descent is commonly used to optimize the parameters of a model to minimize a cost function. The cost function measures the difference between the predicted output of the model and the actual output for a given input. By minimizing the cost function, the model is able to make more accurate predictions."
      ],
      "metadata": {
        "id": "eL-L6sf3diBu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### `Q5`. Describe the multiple linear regression model. How does it differ from simple linear reression?\n"
      ],
      "metadata": {
        "id": "o1n_6AVgZY8m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Multiple linear regression is a statistical model that uses multiple independent variables to predict a single dependent variable. The model assumes that the relationship between the dependent variable and the independent variables is linear. The general equation for multiple linear regression is:\n",
        "\n",
        "  > y = b0 + b1x1 + b2x2 + ... + bnxn <br>\n",
        "  > where y is the dependent variable, x1, x2, ..., xn are the independent variables,<br> b0, b1, b2, ..., bn are the coefficients that represent the impact of each independent variable on the dependent variable\n",
        "  \n",
        "* The multiple linear regression model differs from simple linear regression in that it involves multiple independent variables, while simple linear regression involves only one independent variable. Simple linear regression can be thought of as a special case of multiple linear regression, where there is only one independent variable and the equation reduces to:\n",
        "  > y = b0 + b1x + ε\n",
        "  > where y is the dependent variable,<br> x is the independent variable, <br>b0 is the intercept,<br> b1 is the slope"
      ],
      "metadata": {
        "id": "CnUsWEAFdwsF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### `Q6`. Explain the concept of multicollinearity in multiple linear regression. How can you detect and address this issue?\n",
        "\n"
      ],
      "metadata": {
        "id": "kgN2lwoPZbXc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Multicollinearity is a phenomenon that occurs in multiple linear regression when two or more independent variables are highly correlated with each other. This can cause problems in the regression analysis because it becomes difficult to determine the individual impact of each independent variable on the dependent variable.\n",
        "* To address multicollinearity, one approach is to remove one of the highly correlated independent variables from the analysis. Another approach is to combine the highly correlated independent variables into a single variable through techniques such as principal component analysis. \n",
        "* Additionally, regularization techniques such as Ridge regression and Lasso regression can be used to reduce the impact of multicollinearity by adding a penalty term to the regression equation."
      ],
      "metadata": {
        "id": "2oRykbj8ezHY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### `Q7`. Describe the polynomial regression model. How is it different from linear reression?\n"
      ],
      "metadata": {
        "id": "ddlK-uK9ZcuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Polynomial regression is a statistical model that uses polynomial functions to fit a curve to a set of data points. In contrast to linear regression, which assumes a linear relationship between the independent variable(s) and the dependent variable, polynomial regression models a nonlinear relationship between the variables.\n",
        "\n",
        "* The general equation for a polynomial regression model is:\n",
        "\n",
        "  > y = b0 + b1x + b2x^2 + ... + bnx^n\n",
        "  > where y is the dependent variable,<br> x is the independent variable,,<br>n is the degree of the polynomial, b0, b1, b2, ..., bn are the coefficients, and ε is the error term.\n",
        "\n",
        "* The main difference between linear regression and polynomial regression is that linear regression assumes a linear relationship between the variables, while polynomial regression can capture nonlinear relationships between the variables. In polynomial regression, the curve can take on different shapes, depending on the degree of the polynomial used. Higher-degree polynomials can fit more complex curves to the data, but they also run the risk of overfitting."
      ],
      "metadata": {
        "id": "apGgOyQXhWwb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "#### `Q8`. What are the advantages and disadvantages of polynomial regression compared to linear regression? In what situations would you prefer to use polynomial regression?\n"
      ],
      "metadata": {
        "id": "spFrEqlSZSfq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Advantages of polynomial regression over linear regression include its ability to model nonlinear relationships between variables and its flexibility in fitting a wider range of data patterns. Polynomial regression can also provide more accurate predictions when the relationship between the variables is nonlinear.\n",
        "\n",
        "* Disadvantages of polynomial regression include its susceptibility to overfitting, particularly with higher-degree polynomials, and the difficulty in interpreting the coefficients and determining the specific relationship between the variables.\n",
        "\n",
        "* Polynomial regression is preferred over linear regression when the relationship between the variables is nonlinear, and a curve can better fit the data. Additionally, polynomial regression can be used when there is no theoretical reason to assume a linear relationship between the variables."
      ],
      "metadata": {
        "id": "-I44fsz6jEUR"
      }
    }
  ]
}